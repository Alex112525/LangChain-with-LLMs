{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7uAT7T1SDxpxHP4aiiF3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b24979f74aed48e1816313520a8298dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db4ac960481d4e9790c9a8a2ef11c5d9",
              "IPY_MODEL_72f0e64d6ff4436f955e582d95c8fdc0",
              "IPY_MODEL_d194fce1c29e49269a6fc39e1833107d"
            ],
            "layout": "IPY_MODEL_0b41c7d9e32b4b269cd4dee3f3229172"
          }
        },
        "db4ac960481d4e9790c9a8a2ef11c5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da9b3e882e2a4117bfa1ec499595d10e",
            "placeholder": "​",
            "style": "IPY_MODEL_fdee8aecf9ac49f79f8e92bd1c43fa83",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "72f0e64d6ff4436f955e582d95c8fdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4903d4e18fe04912a901b9902ce941f2",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59a60156f9a54d9ba49714adfd95ed68",
            "value": 5
          }
        },
        "d194fce1c29e49269a6fc39e1833107d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94306de4783b4a31a128f1f447e683f5",
            "placeholder": "​",
            "style": "IPY_MODEL_0c003fbd63964516b440dc25a1743fba",
            "value": " 5/5 [02:14&lt;00:00, 26.03s/it]"
          }
        },
        "0b41c7d9e32b4b269cd4dee3f3229172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9b3e882e2a4117bfa1ec499595d10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdee8aecf9ac49f79f8e92bd1c43fa83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4903d4e18fe04912a901b9902ce941f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a60156f9a54d9ba49714adfd95ed68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94306de4783b4a31a128f1f447e683f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c003fbd63964516b440dc25a1743fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex112525/LangChain-with-LLMs/blob/main/Retrieval_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQc7iNHbg4NF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain pypdf sentence_transformers chromadb einops accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ir4ig71UhwRW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaders = [\n",
        "    PyPDFLoader(\"/content/Attention.pdf\"), # https://arxiv.org/abs/1706.03762\n",
        "    PyPDFLoader(\"/content/Bert.pdf\")      # https://arxiv.org/abs/1810.04805v2\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "  docs.extend(loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "znQqWMu4hyfN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_dir = \"docs/chroma\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding = HuggingFaceEmbeddings(),\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "print(vectordb._collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDtMm5vwh9pF",
        "outputId": "ae19a766-21d1-4401-8b72-4b50e2d0d7eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search with max_marginal_relevance_search()"
      ],
      "metadata": {
        "id": "yrfLpidYjLSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *max_marginal_relevance_search()* function is a method in Chroma that returns documents selected using the maximal marginal relevance. This method optimizes for similarity to query and diversity among the selected documents. The method takes in several parameters such as query, k, fetch_k, lambda_mult, filter and kwargs."
      ],
      "metadata": {
        "id": "APV1jL_ujRor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is attention?\"\n",
        "docs_ss = vectordb.similarity_search(question, k=3)"
      ],
      "metadata": {
        "id": "lfNbGKfAiZWc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_ss[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Lxylj-k0jk0z",
        "outputId": "02f7d915-4102-48c4-8281-817b5ba790b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_ss[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "y-7zJjgvjxQe",
        "outputId": "b31cd2b6-8735-4f91-aad9-2825cfd60d26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a brief explanation of each parameter:\n",
        "\n",
        "* query: Text to look up documents similar to.\n",
        "* k: Number of Documents to return. Defaults to 4.\n",
        "* fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
        "* lambda_mult: Number between 0 and 1 that determines the degree of diversity among selected documents.\n",
        "* filter: A dictionary that specifies the filters for the search."
      ],
      "metadata": {
        "id": "ngZdLWOxj3M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3, lambda_mult=0.8)"
      ],
      "metadata": {
        "id": "UvmDCGTGj0Nz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "q1uWrHGKkHcQ",
        "outputId": "7482c930-dae8-4b42-dc1c-8cfa985e9873"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gsCCTdqDkNt0",
        "outputId": "2d9421ee-1545-45f9-da06-5424b245424b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4-uDhZgkSa6",
        "outputId": "ca6fdfe3-9838-45fc-fb6e-86be6fe553b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'page': 13, 'source': '/content/Attention.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search specifying source"
      ],
      "metadata": {
        "id": "uX62_HFBkeGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_s = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=3,\n",
        "    filter={\"source\": \"/content/Bert.pdf\"}\n",
        ")"
      ],
      "metadata": {
        "id": "osEOLrC4kn9e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs_s:\n",
        "  print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhnU2-tplYhH",
        "outputId": "6234cd50-ec6b-43ce-8b25-221b78db4735"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 2, 'source': '/content/Bert.pdf'}\n",
            "{'page': 2, 'source': '/content/Bert.pdf'}\n",
            "{'page': 2, 'source': '/content/Bert.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_s[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKIHMP4tleGL",
        "outputId": "7b2d128e-0be7-4fff-bccf-49bd09d9cd7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='we will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\\n4We note that in the literature the bidirectional Trans-', metadata={'page': 2, 'source': '/content/Bert.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ContextualCompressionRetriever"
      ],
      "metadata": {
        "id": "ivmWnYvNnSyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **ContextualCompressionRetriever** is designed to improve the answers returned from vector store document similarity searches by better taking into account the context from the query. It wraps another retriever, and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents. This reduces the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgements"
      ],
      "metadata": {
        "id": "0bl3hemjnbvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "AnG3PpB0w1yv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "TfkSRf8cyq1M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model = \"bertin-project/bertin-gpt-j-6B-alpaca\"  # The name of the model to use.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)  # The tokenizer to use to encode and decode text.\n",
        "\n",
        "pipeline = pipeline(\"text-generation\",            # The name of the pipeline to use.\n",
        "                    model=model,                  # The model to use for text generation.\n",
        "                    tokenizer=tokenizer,          # The tokenizer to use to encode and decode text.\n",
        "                    torch_dtype=torch.bfloat16,   # The data type to use for the model's computations.\n",
        "                    trust_remote_code=True,       # Whether to trust remote code when loading the model. This should only be set to True if you trust the source of the model.\n",
        "                    device_map=\"auto\",            # The device to use for the model's computations.\n",
        "                    max_length=512 ,              # The maximum length of the text to generate.\n",
        "                    do_sample=True,               # Whether to use sampling when generating text.\n",
        "                    top_k=2,                      # The number of candidates to keep when sampling.\n",
        "                    num_return_sequences=1,       # The number of text sequences to generate.\n",
        "                    eos_token_id=tokenizer.eos_token_id  # The ID of the end-of-sequence token.\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "b24979f74aed48e1816313520a8298dd",
            "db4ac960481d4e9790c9a8a2ef11c5d9",
            "72f0e64d6ff4436f955e582d95c8fdc0",
            "d194fce1c29e49269a6fc39e1833107d",
            "0b41c7d9e32b4b269cd4dee3f3229172",
            "da9b3e882e2a4117bfa1ec499595d10e",
            "fdee8aecf9ac49f79f8e92bd1c43fa83",
            "4903d4e18fe04912a901b9902ce941f2",
            "59a60156f9a54d9ba49714adfd95ed68",
            "94306de4783b4a31a128f1f447e683f5",
            "0c003fbd63964516b440dc25a1743fba"
          ]
        },
        "id": "1J6Z6u76ncca",
        "outputId": "d4190c7d-d1bc-4c8e-9792-ed36ceeca41b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b24979f74aed48e1816313520a8298dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={\"temperature\": 0.0})\n",
        "compressor = LLMChainExtractor.from_llm(llm)"
      ],
      "metadata": {
        "id": "wv_ANi5snR9o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "p0nZ27D5wc5N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Attention?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfKRxZIQxLjt",
        "outputId": "3ca2faef-af49-491a-e6db-f43cc9ab2fca"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in compressed_docs:\n",
        "  print(20 * \"*** \")\n",
        "  printmd(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "tb5Rj3txxroW",
        "outputId": "a46b7451-1011-4f7c-9709-3f9fb6dbd9e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"question\", \"and\", \"context\"\n\n> Question: What is Attention?\n\n> Context:\n\n### Respuesta:\nLa función de atención es la siguiente: \n\nmapping_query(query, set(keys, values)) \n\nDonde query es la consulta, set es el conjunto de claves y valores, y output es el vector de salida."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"mapping\"\n\"query\",\n\"key-value pairs\"\n\"to\"\n\"an output\"\n\"varies with time and place\"\n\"weighed sum\"\n\n> Question:\n¿Cuál es el número máximo de personas que pueden estar presentes en un aula a la vez?\n\n### Respuesta:\nEl número máximo de personas que pueden estar presentes en un aula a la vez es veinte."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,\ncombined with fact that the output embeddings are offset by one position, ensures that the\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\n3.2.1 Attention\nAn attention function that maps a query to an output that is only partially relevant to the\nquery.\n3.2.2 Attention\nAn attention function that maps a query and an additional key-value pair to an output that is\ncompletly relevant to the query.\n3.3 Attention\nAn attention function that maps a query and an additional key-value pair to an output that is\n3.4 Attention\nAn attention function that maps a query, an additional key-value pair, and an additional\nkey-value pair to an output that is completely relevant to the query.\n3.5 Attention\nAn attention function that maps a query, an additional key-value pair, and an additional\nkey-value pair, and an additional key-value pair to an output that are all completely\n3.6 Attention\nAn attention function that maps a query, an additional key-value pair, an additional key-value\npair, and an additional key-value pair to an output that are all partially relevant to the\n\n### Respuesta:\n3.6.1 Attention\nAn attention function that maps a query, an additional key-value pair,"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"mapping\"\n\"query\", \"set of keys and values\"\n\"output\"\n\"vector\"\n\n> Respuesta:\nLa función de atención es una función que asigna un peso a cada palabra en la consulta y en el conjunto de pares de clave-valor, de manera que la salida sea un vector que representa la suma ponderada de las palabras en la consulta y el conjunto de pares de clave-valor."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever_mmr = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
        ")"
      ],
      "metadata": {
        "id": "5huFj4WXy6B4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the positional encoding?\"\n",
        "compressed_docs_mmr = compression_retriever_mmr.get_relevant_documents(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbSS0xPCzFXQ",
        "outputId": "75448a6c-4806-4fa4-e66b-c1a8f73e1868"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in compressed_docs_mmr:\n",
        "  print(20 * \"** \")\n",
        "  printmd(doc.page_content)\n",
        "  print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "Ji7xcf3KzRrw",
        "outputId": "76948920-412d-4a76-ab49-2f65b8ba65ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "sub-layer, in, decoder, stack, to, prevent, positions, from, attending, subsequent, positions\n\n### Respuesta:\nLa encriptación de posición es una técnica en la que los datos se codifican en función de la posición en un vector o matriz, en lugar de hacerlo en función del valor absoluto o del valor final. Esta técnica se utiliza para evitar que los datos sean visibles durante la transmisión o el almacenamiento. La encriptación de posición es una técnica común utilizada en aplicaciones de seguridad, como la autenticación de contraseñas y la encriptación de datos."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 2, 'source': '/content/Attention.pdf'}\n",
            "** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "We use a scale-free embedded embeddings of the input to represent the text.\n\nWe use a scale-free softmax to represent the text.\n\n> Question: What is the position of the capital \"C\"?\n> Context:\n### Entrada:\nThe capital of France is Paris.\n\n### Respuesta:\nLa capital de Francia es París."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 4, 'source': '/content/Attention.pdf'}\n",
            "** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "chose this function because we hypothesised it would allow the model to easily learn to attend\nby absolute positions.\n\n### Respuesta:\nLa codificación posicional es una técnica de codificación en la que las letras, números y símbolos se representan mediante posiciones relativas en una matriz. Se utiliza comúnmente en tareas de aprendizaje automático supervisado, como la cl"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 5, 'source': '/content/Attention.pdf'}\n",
            "** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** ** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Encoder:\n6 layers, 2 sub-layers per layer\n\nDecoder: \n2 sub-layers\n3rd sub-layer: Multi-head attention\nResidual connections:\n[ 11, 1, 1, 1, 1, 1]\n\n### Output:\n\nModel: \n\nInputs:\n[X, Y, Z, A"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 2, 'source': '/content/Attention.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "1EwOCfzk32lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling"
      ],
      "metadata": {
        "id": "ujKFq7BI4PC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/Attention.pdf\")\n",
        "pages = loader.load()\n",
        "all_pages = [p.page_content for p in pages]\n",
        "joined_pages = \" \".join(all_pages)\n",
        "\n",
        "#Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "splits = text_splitter.split_text(joined_pages)"
      ],
      "metadata": {
        "id": "yRMqW0Uq2W_D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import TFIDFRetriever"
      ],
      "metadata": {
        "id": "yV0PmCR04SDL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
      ],
      "metadata": {
        "id": "sd9Wtabk4bmg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is attention?\""
      ],
      "metadata": {
        "id": "as3fWwZg3WfF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_tf = tfidf_retriever.get_relevant_documents(question)\n",
        "for doc in docs_tf:\n",
        "  print(\"*** \" * 20)\n",
        "  printmd(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "i8fYkrLE4x88",
        "outputId": "b88c4cb4-8166-4393-b56b-3c84e94f23af"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "be\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13 Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14 Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "layers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3 Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence"
          },
          "metadata": {}
        }
      ]
    }
  ]
}